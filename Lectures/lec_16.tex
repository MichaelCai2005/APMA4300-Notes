\chapter{Numerical Differentiation}
\lecture{15}{25 Mar 2025}{10:10}
\section{Definitions of Differentiation}
\begin{definition}
    We know for the definition of a derivative is given as 
    Forward Difference:
    \[
        f(x) = \lim_{h \to 0} \frac{f(x+j) - f(x)}{h}
    \]    
    Backwards Difference:
    \[
        \lim_{h \to 0} \frac{f(x) - f(x-h)}{h}
    \]
    Central Difference
    \[
        \lim_{h \to 9}  \frac{f(x+h) - f(x-h)}{2h}
    \]
\end{definition}
We define the following wherefor numerical differentiation that \(h\) is small. Note the above definitions are going to be different types 
of numerical differentiations. To check which gives the best approximation, we can Taylor expand s.t 
\[
    f(x+h) = f(x) + f^{\prime} (x) h + \frac{1}{2 } f^{\prime} (x) h^{2} +\dots 
\]
\[
    f(x-h) =  f(x) + f^{\prime} (x)(-h) + \frac{1}{2} f^{\prime\prime} (x) h^{2} 
\]
\[
    \implies \frac{f(x+h)-f(x)}{h} = f^{\prime} (x) +\frac{1}{2}f^{\prime\prime} (x) h
\]
\[
    \frac{f(x) - f(x-h)}{h} = f^{\prime} (x) - \frac{1}{2}f^{\prime\prime} (x) h 
\]
\[
    \frac{f(x+h)-f(x-h)}{2h} = f^{\prime} (x) + \frac{1}{3!} f^{\prime\prime\prime} h^{2} 
\]
where we see the central difference gives a smaller error where \(h^{2} \ll 1\) thus the error term shows up only when 
\(f^{\prime\prime\prime} \) is very large. 

\section{General Approach \(n+1\) Point Formula}
Assume we want to approximate \(n+1\)points around \(x\) to approximate the derivative \(f^{\prime} (x)\). The Lagrange polynomial approximation is given as 
\[
    f(x) = \sum_{k=0}^{n} f(x_k) L_{nk} (x) + \frac{f^{(n+1)} (\zeta)}{(n+1)!} \prod_{k=0}^n (x-x_k)
\]   
We can simply differentiate to have the new expression as 
\[
    f^{\prime} (x) = \sum_{k=0}^{n} f(x_k) L^{\prime} _{nk} (x) + \frac{f^{(n+1)} (\zeta)}{(n+1)!}  \left[ \prod_{k=0}^n (x-x_k) \right]^{\prime} 
\]
This is the generic method of differentiation from interpolation which the second term represents the error term. 
\begin{remark}
    If we naively use this method for points spread apart, the second error term \(\epsilon \) will be very large. This is the case for the 
    taylor expansion too where the points are very spread apart. 
\end{remark}
For the general formulation to find the derivatives at \(x_j\) we simply plug in to the lagrange interpolation and take its derivative. We then ask the question
of whether or not we recover the finite difference approximation for the setup of \(x_0, x_1, x_2\). 

\subsection{Three and Five Point Formula}
Consider three points difference scheme to approximate the derivative. We can rewrite this in terms of \(x\) and \(h\) such that 
they are spatially selected to be uniform where we can plug in for the the points of \(x-h\), \(x\), and \(x+h\). If we calculate the 
Lagrangian interpolation our result for a uniform mesh is given. 
\[
    f^{\prime} (x_{0} ) = \frac{1}{2h}\left[ 
        -3 f(x_0) + 4f(x_1) - f(x_2)
     \right] + \frac{f^{(3)} (\zeta_0)}{3!} h^{2} 
\]   
\[
    f^{\prime} (x_1) = \frac{1}{2h} \left[ -f(x_0) + f(x_2) \right] + \frac{f^{\prime\prime\prime} (\zeta_1)}{3!}h^{2} 
\]
\[
    f^{\prime} (x_2) = \frac{1}{2h} \left[ 
        f(x_0) -4 f(x_1) +3 f(x_2)
     \right] + \frac{f^{\prime\prime\prime} (\zeta_2)}{3!} h^{2} 
\]
Similarly there exists a five point formula that does the same thing with a higher order accuracy. 